# Image Classification
## Summary
1. What is image classification?
2. Simpliest method: Nearest Neighbor Classifier.
3. Pros and Cons of NNC.
4. Validation set, cross-validation.

## 1. Image Classification
Task：assign an input image one label from a fixed set of categories.\
Example：give a image of cat， find a label from {cat,dog, hat, mug} for the image.\
Note: if a image stored with x pixel wide, y pixel tall and z color channels, then there total xyz numbers. Every number is between 0-255. 

### Some challenges:
* viewpoint variation：同一个物体可以从不同的角度拍照片
* scale variation： 在现实中，同一个物体可以有不同的大小，在照片中体现的就不一样
* deformation：比如一只哈士奇，不一定是安静的站着拍照，可以会有很多不能的奇怪的形状的动作的图片
* occlusion：一只哈士奇躲在沙发里的照片，不能看到哈士奇的完整照片
* illumination： 照片可能在一个昏暗的环境里
* background clutter： 当一只斑马躺在斑马线上
* intra—class variation： 同一个椅子，有很多中不同的款式💺

### Pipeline:
1. input：需要一组n个图片，且每个图片有k个标签中的一个作为training data。
2. learning： use training set to learn what every one of the class looks like.
3. evaluation： evaluate the quality of the classifier by asking it to predict labels for a new set of images that it never seen before. (predictor & ground truth).

Data-driven approach: we can't write the code like simple python code, here, we need provide computer with many examples of each class and then develop learning algorithm that look at these examples and learn about the visual appearance of each class.

## 2. Nearest Neighbor Classifier
Idea: take a test image, compare it to every single one of the training images by compute there difference, find the smallest difference one in training data, and the label of this one is the final predictor. 

Method: sum the difference (l1 norm or l2 norm) between two images directly.

### Extension: K - Nearest Neighbor Classifier
Instead of finding the single closest image in the training set, will find top k closest images,and have them vote on the label of the test image.

## 3. Pros and Cons of NNC
Pros: simpel to implement and understand, don't need to train data.\
Cons: computational cost at test time. When change one image(shifted, messed up, darkened), distance will change lot. When we classifier images, we need consider semantic content not only color/ background.

## 4. Validation set, cross-validation.
Hyperparameter: not obvious what values/setting one should choose. Learn from data.

Method to tune hyperparameter: we can seperate traning data into two parts, a smaller training data set and a **validation set(fake test set)**. We can use validation set to tune hyperparameter. 

Note: we can use test set for tweaking hyperparameter. If we do, will cause two problems, overfit and too optimistc. 

Example: we have 100 data. \
100 data = (90 training + 10 testing)
         = ((80 training + 10 validaton) + 10 testing)
         
### Cross-validation: 
when the size of training data is small, can use cross-validation to seperate training data into k parts. k-1 of them to train and one part as validation. Then choose another part as validation, rest as training data. \
Normally, we can set k as 3, 5, 10. 
If there are lots of hyperparameter, we can use bigger validation split. Else, we use cross-validation.

         
         


