# Image Classification
## Summary
1. What is image classification?
2. Simpliest method: Nearest Neighbor Classifier.
3. Pros and Cons of NNC.
4. Validation set, cross-validation.

## 1. Image Classification
Taskï¼šassign an input image one label from a fixed set of categories.\
Exampleï¼šgive a image of catï¼Œ find a label from {cat,dog, hat, mug} for the image.\
Note: if a image stored with x pixel wide, y pixel tall and z color channels, then there total xyz numbers. Every number is between 0-255. 

### Some challenges:
* viewpoint variationï¼šåŒä¸€ä¸ªç‰©ä½“å¯ä»¥ä»ä¸åŒçš„è§’åº¦æ‹ç…§ç‰‡
* scale variationï¼š åœ¨ç°å®ä¸­ï¼ŒåŒä¸€ä¸ªç‰©ä½“å¯ä»¥æœ‰ä¸åŒçš„å¤§å°ï¼Œåœ¨ç…§ç‰‡ä¸­ä½“ç°çš„å°±ä¸ä¸€æ ·
* deformationï¼šæ¯”å¦‚ä¸€åªå“ˆå£«å¥‡ï¼Œä¸ä¸€å®šæ˜¯å®‰é™çš„ç«™ç€æ‹ç…§ï¼Œå¯ä»¥ä¼šæœ‰å¾ˆå¤šä¸èƒ½çš„å¥‡æ€ªçš„å½¢çŠ¶çš„åŠ¨ä½œçš„å›¾ç‰‡
* occlusionï¼šä¸€åªå“ˆå£«å¥‡èº²åœ¨æ²™å‘é‡Œçš„ç…§ç‰‡ï¼Œä¸èƒ½çœ‹åˆ°å“ˆå£«å¥‡çš„å®Œæ•´ç…§ç‰‡
* illuminationï¼š ç…§ç‰‡å¯èƒ½åœ¨ä¸€ä¸ªæ˜æš—çš„ç¯å¢ƒé‡Œ
* background clutterï¼š å½“ä¸€åªæ–‘é©¬èººåœ¨æ–‘é©¬çº¿ä¸Š
* intraâ€”class variationï¼š åŒä¸€ä¸ªæ¤…å­ï¼Œæœ‰å¾ˆå¤šä¸­ä¸åŒçš„æ¬¾å¼ğŸ’º

### Pipeline:
1. inputï¼šéœ€è¦ä¸€ç»„nä¸ªå›¾ç‰‡ï¼Œä¸”æ¯ä¸ªå›¾ç‰‡æœ‰kä¸ªæ ‡ç­¾ä¸­çš„ä¸€ä¸ªä½œä¸ºtraining dataã€‚
2. learningï¼š use training set to learn what every one of the class looks like.
3. evaluationï¼š evaluate the quality of the classifier by asking it to predict labels for a new set of images that it never seen before. (predictor & ground truth).

Data-driven approach: we can't write the code like simple python code, here, we need provide computer with many examples of each class and then develop learning algorithm that look at these examples and learn about the visual appearance of each class.

## 2. Nearest Neighbor Classifier
Idea: take a test image, compare it to every single one of the training images by compute there difference, find the smallest difference one in training data, and the label of this one is the final predictor. 

Method: sum the difference (l1 norm or l2 norm) between two images directly.

### Extension: K - Nearest Neighbor Classifier
Instead of finding the single closest image in the training set, will find top k closest images,and have them vote on the label of the test image.

## 3. Pros and Cons of NNC
Pros: simpel to implement and understand, don't need to train data.\
Cons: computational cost at test time. When change one image(shifted, messed up, darkened), distance will change lot. When we classifier images, we need consider semantic content not only color/ background.

## 4. Validation set, cross-validation.
Hyperparameter: not obvious what values/setting one should choose. Learn from data.

Method to tune hyperparameter: we can seperate traning data into two parts, a smaller training data set and a **validation set(fake test set)**. We can use validation set to tune hyperparameter. 

Note: we can use test set for tweaking hyperparameter. If we do, will cause two problems, overfit and too optimistc. 

Example: we have 100 data. \
100 data = (90 training + 10 testing)
         = ((80 training + 10 validaton) + 10 testing)
         
### Cross-validation: 
when the size of training data is small, can use cross-validation to seperate training data into k parts. k-1 of them to train and one part as validation. Then choose another part as validation, rest as training data. \
Normally, we can set k as 3, 5, 10. 
If there are lots of hyperparameter, we can use bigger validation split. Else, we use cross-validation.

         
         


