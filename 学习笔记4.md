# backpropagation
Task: given some function f, X is a vector, compute gradient of f at x. 

Application: in neural network, f is loss function L, x consists by training data and weight. 

Example: in SVM loss function, inputs are training data, W and b. But we think training data as constant, so we only need to 
        think the gradient for W and b.
        
Meaning for derivatives: f(x,y) = xy, df/dx = y, then is x change a little, df/dx is the change rate for the whole function.

Example: x =4, y = -3, then df/dx = y = -3. if x change one, then the f will change -3. 

Three normal gradient gate:
1. add gate:  takes the gradient on its output and distributes it equally to all of its inputs, regardless of what their 
values were during the forward pass.
2. max gate: gradient (unchanged) to exactly one of its inputs (the input that had the highest value during the forward pass). 
3. multiply gate: Its local gradients are the input values (except switched), and this is multiplied by the gradient on its output during 
the chain rule. 


**Note: only need to know how to compute gradient!**
