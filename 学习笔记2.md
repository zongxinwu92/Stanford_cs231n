# Linear Classification
## Summary:
1. Score function.
2. Loss function.
3. Multiclass support vector machine classifier (SVM).
4. Softmax classifier.
5. Difference

## 1. Score function
Idea: we can find a mapping from image to class(a vector, contains all info about each class), then analyze the class vector.\
Assume N images, K classes, then we nned to define a function f(xi, W, b) = Wxi + b. (xi with all its pixels flattened out to a single column vector).  \
**W: weight**, K * D matrix. \
**b: bias vector**, K * 1 vector. \
**f: score class**, K * 1 vector.(totoal K class, each score for a class) \
Goal: compute scores match the ground truth labels across the whole training data.
[image][]
Note: if change row of W, the class line will retate. if change b, the class line will translate. Each row of W is a **template for one class**.(W can been think as an image). b can be included in W: f = (W b) (xi, 1)'. 

## 2. Loss function(cost function)
Idea: if loss function is high, it's poor job of classifying the training data. It quantifier our unhappyness for predictor on the training data. \
There two kinds of loss function, multiclass SVM and softmax.

## 3. Multiclass SVM
Idea: want the correct class for each image to have score higher than the incorrect classes by some fixed margin Δ. 
Formula: for image xi, it has label yi,**Note: yi is ground truth(it's given), not the biggest value in the value class** if we calculate f(xi, W), we will get a vector s. The score for j-th class is 
sj = f(xi, W)j. Then  **data loss** function of Multiclass SVM method is
Li=∑ (j≠yi) max(0,sj−syi+Δ). (i.e. all sum of wrong class differece and some margin)

Example: score class s=[13, -7, 11]. The ground truth index(yi) = 0, Δ = 10. Then 
L = max(0, -7-13+10) + max(0, 11-13+10) 
  = max(0, -20 + 10) + max(0,-2 + 10)
Analyze: since -20 is more less than margin, no loss, so loss = 0. Althogh 11 is less then 13, doesn't reach the margin. So there still some loss.  That's the score of correct label should bigger than incorrect at least margin.

**Regularization**: assume we have database and W classify correctly, then Li = 0 for all i. Now, W is not unique, since λW has same result. Now, we can add a **regularization loss**.
regularization R(W) = ∑i∑j w2 (i.e square of each num in W)

Then L=(1/N) ∑ (Li+λR(W))

Practical Consideration:
1. W has much more influence then b. 
2. Binary support vector machine is a special case of multiclass SVM. 
3. Other SVM: one-vs-one SVM, all-vs-all SVM, structured SVM. 

## 4. Softmax 
f(xi, W) = Wxi, then we consider the score result as unnormalized log probabilities for each class.
**cross-entropy loss**: Li= −log(efyi * ∑j efj)or equivalently Li=−fyi+ log∑j e fj
**softmax function**: fj(z)=e zj ∑k e zk

Some calculation tips.

## 5. Difference
Softmax classifier, the SVM is a more local objective, which could be thought of either as a bug or a feature. Consider an example that achieves the scores [10, -2, 3] and where the first class is correct. An SVM (e.g. with desired margin of Δ=1) will see that the correct class already has a score higher than the margin compared to the other classes and it will compute loss of zero. The SVM does not care about the details of the individual scores: if they were instead [10, -100, -100] or [10, 9, 9] the SVM would be indifferent since the margin of 1 is satisfied and hence the loss is zero. However, these scenarios are not equivalent to a Softmax classifier, which would accumulate a much higher loss for the scores [10, 9, 9] than for [10, -100, -100]. In other words, the Softmax classifier is never fully happy with the scores it produces: the correct class could always have a higher probability and the incorrect classes always a lower probability and the loss would always get better. However, the SVM is happy once the margins are satisfied and it does not micromanage the exact scores beyond this constraint. This can intuitively be thought of as a feature: For example, a car classifier which is likely spending most of its “effort” on the difficult problem of separating cars from trucks should not be influenced by the frog examples, which it already assigns very low scores to, and which likely cluster around a completely different side of the data cloud.

