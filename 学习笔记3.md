#Optimization: Gradient.
## Summary
1. Task for optimization.
2. Visualizing the loss function.
3. Some brain-dead methods to optimaize weight W.
4. Gradient descent for weight W  (☑️).

## 1. Task for optimization
There are 3 steps for image classification: 
1. **score function** :mapping image to a score value.
2. **loss function**: measured the quality of a particular set of parameters based on how well the induced scores 
agreed with the ground truth labels in the training data\.
3. **optimization**: find weight W that minimize loss function. 

## 2. Visualizing the loss function.
模型理解：类似一个登山者在一个山坡上（对于任意weight的loss function），现在他想要找到一条最快的路到山底（找到一个最大的gradient或者说
找到最陡的一条路通向山底，也就是loss function最小的地方）。


## 3. Some brain-dead methods to optimaize weight W
1. random search: try as many as possibile W, record the one which has minimal loss function. 
2. random local search: start with random W, then give it a random perturbations δW. Calculate L(W+δW), 
if L is less, then update weight.

## 4. Gradient descent for weight W  (☑️)
**Idea**: we can compute the best direction along which we should change our weight vector that is mathematically guaranteed 
to be the direction of the steepest descend (at least in the limit as the step size goes towards zero). 
This direction will be related to the **gradient of the loss function**.\
**Gradient**: a generalization of slope for functions that take vector of numbers. A vector of slopes (more commonly 
referred to as **derivatives**) for each dimension in the input space.
When the functions of interest take a vector of numbers instead of a single number, we call the derivatives **partial derivatives**, and the gradient is simply the vector of partial derivatives in each dimension.

### Computation for gradient
1. **numerical gradient**: slow, approximate but easy way (从微积分原理上出发)\
    step 1: define eval_numerical_gradient(f, x) function, (从原理出发)will return gradient of f at each node x, the dimension\               will same as x.\
    step 2:  
    

2. **analytic gradient**:fast, exact but more error-prone way （直接求导）




