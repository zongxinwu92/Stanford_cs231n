# Optimization: Gradient.
## Summary
1. Task for optimization.
2. Visualizing the loss function.
3. Some brain-dead methods to optimaize weight W.
4. Gradient descent for weight W  (☑️).

## 1. Task for optimization
There are 3 steps for image classification: 
1. **score function** :mapping image to a score value.
2. **loss function**: measured the quality of a particular set of parameters based on how well the induced scores 
agreed with the ground truth labels in the training data\.
3. **optimization**: find weight W that minimize loss function. 

## 2. Visualizing the loss function.
模型理解：类似一个登山者在一个山坡上（对于任意weight的loss function），现在他想要找到一条最快的路到山底（找到一个最大的gradient或者说
找到最陡的一条路通向山底，也就是loss function最小的地方）。


## 3. Some brain-dead methods to optimaize weight W
1. random search: try as many as possibile W, record the one which has minimal loss function. 
2. random local search: start with random W, then give it a random perturbations δW. Calculate L(W+δW), 
if L is less, then update weight.

## 4. Gradient descent for weight W  (☑️)
**Idea**: we can compute the best direction along which we should change our weight vector that is mathematically guaranteed 
to be the direction of the steepest descend (at least in the limit as the step size goes towards zero). 
This direction will be related to the **gradient of the loss function**.\
**Gradient**: a generalization of slope for functions that take vector of numbers. A vector of slopes (more commonly 
referred to as **derivatives**) for each dimension in the input space.
When the functions of interest take a vector of numbers instead of a single number, we call the derivatives **partial derivatives**, and the gradient is simply the vector of partial derivatives in each dimension.

### Computation for gradient
1. **numerical gradient**: slow, approximate but easy way (从微积分原理上出发)\
step 1: define eval_numerical_gradient(f, x) function, (从原理出发)will return gradient of f at each node x, the dimension               will same as x.
     
step 2:  calculate gradient(L, W), L is loss function, W is weight.
    
step3 : for different step_size, can get a new weight, then the loss function of wight. (step_size impacts significantly               on the finial loss, a hyperparameter)(站在山顶，一步跨的非常远，可能会超过了最低的山谷， 一步非常近，还没到山谷，只有恰到好处才能到达)
    

2. **analytic gradient**:fast, exact but more error-prone way （直接求导）
Differentiate the loss function with respect to the weights directly. (pay attention to the index of W). After we get all
result for this (i.e. gradient for each node in wight, then we can choose different step_size)

**Gradient Descent**: compute the gradient of the loss function, the procedure of repeatedly evaluating the gradient and then performing a parameter update.




