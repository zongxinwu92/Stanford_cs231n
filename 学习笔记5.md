# Neural nerworks: setting up the architecture.
## Summary:
1. some examples
2. One neural model
3. Activation function
4. Neural network architecture

## 1. example
look at following two models:

s = Wx compute scores for different visual categories

s = W2 (max(0, W1x))

W1: transform image to 100 dimensions vector

max: non-linearity (activation function). There lots kind of non-linearity. max is common and simple.

W2: transform size of data same as size of categories

Max function is important, we get wiggle from it. W1 and W2 can be learned from stochastic gradient descent.

**Note:** size of the intermediate hidden vector = hyperparameter of the network.\
Example: s = W3(max(0,W2 (max(0, W1x)))) is three-layer.

## 2. one neural model.
Forward-propagating a single neuron(è§ç¬”è®°)ã€‚

We can turn one neural model into a linear classifier by a appropriate loss function. \
Ex: 
1. binary softmax classifier: use cross-entropy loss.
2. binary SVM classifier: use max-margin hinge loss.
3. regularization interpretation: regularization loss have effect of driving W towrds 0 after every parameter update.(???)

## 3. Activation function
1. sigmoid
2. Tanh
3. ReLU
4. Leaky ReLu
5. maxout

æœ€ä½³ä½¿ç”¨é¡ºåºï¼š ReLU â¡ï¸ leaky ReLU or maxout â¡ï¸ Tanh\
**never use sigmoid**


## 4. Neural network architecture
Neural network: neurons connected in an acylic graph.\
Fully - connected layer: neurons between two adjacent layers are fully pairwise connected.\
Some rules for the NN:
1. Naming: hidden layer (number of hyperparameter); N-layer (not counting input layer); single layer neural network is logistic
regression or SVM or ANN.
2. output-layer: don't have activation function.
3. Size of neural network: use # neuron or #parameter.\
Ex: for a following single-layer neural network, \
â­•ï¸        â­•ï¸        â­•ï¸\
â­•ï¸        â­•ï¸        â­•ï¸\
â­•ï¸        â­•ï¸    
ğŸˆ³ï¸        â­•ï¸  \
There 6 neurons (not including input), 3 x 4 + 4 x 2 = 20 weights, 4 + 2 = 6 biases. 
Total 26 parameters. \

example of feed-forward computation:\
â­•ï¸        â­•ï¸        â­•ï¸          â­•ï¸\
â­•ï¸        â­•ï¸        â­•ï¸ \
â­•ï¸        â­•ï¸        â­•ï¸\
ğŸˆ³ï¸        â­•ï¸        â­•ï¸
 
Dimension for each variable:\
input : 3 * 1 \
W1: 4 * 3 \
W2: 4 * 4 \
W3: 1 * 4 

f = sigmoid(x) \
x = np.random.randn(3,1) \
h1  = f(np.dot(W1, x) + b1) \
h2  = f(np.dot(W2, x) + b2) \
out = np.dot(W1, x) + b3 


